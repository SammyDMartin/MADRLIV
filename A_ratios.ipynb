{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd08fac594bfae6525c0c41b4041d2d72effa188cc8ead05f81b1fab2bb098927fb",
   "display_name": "Python 3.7.6 64-bit (conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MADRLIV import plot_singlepref\n",
    "from MADRLIV import unique_nontrival_winner\n",
    "\n",
    "from MADRLIV import util_from_ranks\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mems = [False] #types of agent memory to record\n",
    "\n",
    "agent_types = ['tabular'] #types of agent\n",
    "\n",
    "N_pref=2500\n",
    "\n",
    "DP=False\n",
    "if N_pref>1:\n",
    "    DP = True\n",
    "\n",
    "#produce the preference profile\n",
    "C = 3 #must be 3\n",
    "V = 7\n",
    "\n",
    "CV = (C,V)\n",
    "\n",
    "\n",
    "alpha = 0.1\n",
    "nt = 5\n",
    "epslen = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vote_histories = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(FloatProgress(value=0.0, max=2500.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6f448e241334452af9156ee38210883"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for tests in tqdm(range(N_pref)):\n",
    "    opt,vr, metric_results,plurality_results = unique_nontrival_winner(CV[0],CV[1],'borda',restrict=True)\n",
    "    pp = [opt,vr]\n",
    "\n",
    "    results_mix = plot_singlepref(fold=None,mems=mems,agent_types=agent_types,pref_profile=pp,agent_alpha=alpha,N_tests=nt,percent=10,metric='borda_score',eps_len=epslen,updateinterval=2,disablep=DP)\n",
    "\n",
    "    for k in results_mix.keys():\n",
    "        reslist = results_mix[k]\n",
    "        scraped = [result[1] for result in reslist]\n",
    "        results_mix[k] = scraped\n",
    "\n",
    "    if vote_histories == {}:\n",
    "        vote_histories = {key : [] for key in results_mix.keys()}\n",
    "\n",
    "    for k in results_mix.keys():\n",
    "        vote_history = [(v.vote_history,v.vote_winners,opt,vr) for v in results_mix[k]]\n",
    "        vh = vote_histories[k]\n",
    "        vh = vote_history + vh\n",
    "        vote_histories[k] = vh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "no stored variable or alias vote_histories\n"
     ]
    }
   ],
   "source": [
    "#%store vote_histories\n",
    "#%store -r vote_histories\n",
    "#vote_histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leave alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = (False, 'tabular')\n",
    "hist = vote_histories[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#“A-ratio,” which is the fraction of instances where action A was played out of all instances where it was available.\n",
    "#WHERE IT WAS AVAILABLE\n",
    "#for the 3 candidate situation:\n",
    "\n",
    "#TRT - Truthful action - voting for candidate with utility 1.00\n",
    "def TRT(utility):\n",
    "    return float(utility==1.00)\n",
    "\n",
    "#CMP - Compromise action - voting for utility 0.5 when utility 1.00 (most preferred option) is ranked last in poll (is losing consistently over a large enough past interval??)\n",
    "\n",
    "\n",
    "#LB - Leader bias - voting for the leader of the poll = (consistent winner over a large enough past interval) that is either 0.5 or 0.25 utility\n",
    "def LB(utility,poll):\n",
    "    if utility < 1.00:\n",
    "        if poll[utility] == max(poll.values()):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "#DOM - Dominated actions - another action yields a higher expected utility \"under very weak assumptions\" = voting for the worst possible candidate (i.e. the last ranked option)??\n",
    "#need to investigate - does 'no rational movitation' ONLY mean voting for last ranked option? Or could voting for 2nd option be DOMinated (probably no under 'very weak assumptions')\n",
    "\n",
    "def DOM(utility,poll):\n",
    "    if utility == 0.25:\n",
    "        #action is dominated if you choose worst possible option\n",
    "        return True\n",
    "    else:\n",
    "        if utility == 0.5:\n",
    "            #if you choose 2nd best option when maximised score is ahead in the polls\n",
    "            score_choice = poll[0.5]\n",
    "            score_max = poll[1.00]\n",
    "            if score_max>=score_choice:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def CMP(utility,poll):\n",
    "    if utility == 0.5:\n",
    "        score_choice = poll[0.5]\n",
    "        score_max = poll[1.00]\n",
    "        if score_choice>score_max:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "#note that DOM and LB can go together, as can CMP and Leader bias\n",
    "\n",
    "#so we have DLB (dominated leader bias) and CLB (compromise leader bias) for if you're voting for the poll leader (consistent leader) as a dominated or a compromise action\n",
    "\n",
    "#NEED - history of vote winners to estimate LB. 'poll' has no direct equivalent\n",
    "\n",
    "#In the second game a voter directly\n",
    "#observes the current votes of her peers, but does not know\n",
    "#how they will vote eventually at the final round (or when\n",
    "#will the final round arrive).\n",
    "\n",
    "#suggests the A ratios are based on past votes\n",
    "\"\"\"\n",
    "Since only one voter may change her vote at each step\n",
    "at, at+1 differ by at most one entry\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "we computed and analyzed A-ratios in the same way as\n",
    "we did for one-shot voting, except that instead of a poll we used the current voting profile\n",
    "At. More specifically, we counted each step by player i as a separate decision, classifying it\n",
    "into one of six scenarios as in Table 3 and checking the action classes from A to which it applies\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#we also want to determine move types\n",
    "\n",
    "\"\"\"\n",
    "Following [18], we denote a compromise move as a change\n",
    "in vote to a less-preferred candidate, and an opportunity\n",
    "move as a change in vote to a more-preferred candidate. We\n",
    "denote a stay move as no change in voting compared to the\n",
    "previous round.\n",
    "\"\"\"\n",
    "def determine_move_type_from_utility_of_vote_assigned(utility,utility_last,move):\n",
    "    #takes in utility score of the chosen option for this and last round - to see if agent moved or stayed and if they compromised or not\n",
    "    if utility_last == None:\n",
    "        #first move\n",
    "        out= 0\n",
    "    elif utility == utility_last:\n",
    "        #stay move\n",
    "        out= 0\n",
    "    elif utility > utility_last:\n",
    "        #opportunity move\n",
    "        out= 1\n",
    "    elif utility < utility_last:\n",
    "        #compromise move\n",
    "        out= -1\n",
    "    elif (utility,utility_last) == (None,None):\n",
    "        #fails\n",
    "        raise InterruptedError\n",
    "    \n",
    "    if move == \"opportunity\":\n",
    "        return(out==1)\n",
    "    elif move == \"stay\":\n",
    "        return(out==0)\n",
    "    elif move == \"compromise\":\n",
    "        return(out==-1)\n",
    "    else:\n",
    "        print(move)\n",
    "        print(\"Invalid move/Atype\")\n",
    "        raise InterruptedError\n",
    "    \n",
    "\n",
    "def determine_A_type_from_utility_of_vote_assigned(utility,utility_last,winner_history, util_of_vote,A_type,cutoff=10):\n",
    "    if len(winner_history)>cutoff:\n",
    "        reward_history = [util_of_vote[w] for w in winner_history]\n",
    "        reward_history = reward_history[int(-1*cutoff):]\n",
    "        poll = {idv : 0 for idv in [1.00,0.5,0.25]}\n",
    "        for i in reward_history:\n",
    "            poll[i] = poll.get(i, 0) + 1\n",
    "\n",
    "        #print()\n",
    "        #print(utility,poll)\n",
    "        #print(\"TRT:{}\\nDOM:{}\\nLB:{}\\nCMP:{}\".format(float(TRT(utility)),float(DOM(utility,poll)),float(LB(utility,poll)),float(CMP(utility,poll))))\n",
    "        #print()\n",
    "      \n",
    "\n",
    "        if A_type == \"TRT\":\n",
    "            return float(TRT(utility))\n",
    "        elif A_type == \"DOM\":\n",
    "            return float(DOM(utility,poll))\n",
    "        elif A_type == \"CMP\":\n",
    "            return float(CMP(utility,poll))\n",
    "        elif A_type == \"LB\":\n",
    "            return float(LB(utility,poll))\n",
    "        elif A_type == \"DLB\":\n",
    "            return float(DOM(utility,poll) and LB(utility,poll))\n",
    "        elif A_type == \"CLB\":\n",
    "            return float(LB(utility,poll) and CMP(utility,poll))\n",
    "        else:\n",
    "            return determine_move_type_from_utility_of_vote_assigned(utility,utility_last,A_type)\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def convert_to_numbers(winlist,options):\n",
    "    return np.array([opt.index(i) for i in winlist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AP = 50\n",
    "def moving_average(a, n=1):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_action_type(history,A_type,poll_cutoff=10):\n",
    "    final_results = [np.zeros(epslen+1) for v in range(len(vr))]\n",
    "\n",
    "    for history_comps in tqdm(history,desc=A_type):\n",
    "        run,options_round,vr_round = history_comps[0],history_comps[2],history_comps[3]\n",
    "        entire_winner_history = convert_to_numbers(history_comps[1],options=options_round)\n",
    "        \n",
    "        agent_vote_utilities = [np.zeros(epslen+1) for v in range(len(vr_round))]\n",
    "        \n",
    "        for eps_number,step in enumerate(run):\n",
    "            if eps_number>1:\n",
    "                winner_history = entire_winner_history[:eps_number-1]\n",
    "            else:\n",
    "                winner_history = []\n",
    "\n",
    "            for agent_num,vote in enumerate(step):\n",
    "                agents_prefs = vr_round[agent_num]\n",
    "                #need to store specific agent rank not overall!!\n",
    "                util_of_vote = util_from_ranks(rank_list=agents_prefs,options=options_round,use_exp=True)\n",
    "                \n",
    "                utility = util_of_vote[vote]\n",
    "                \n",
    "                #store for this run agent utility and get last utility\n",
    "                v=agent_vote_utilities[agent_num]\n",
    "                if eps_number == 0:\n",
    "                    utility_last = None\n",
    "                else:\n",
    "                    utility_last = v[eps_number-1]\n",
    "                v[eps_number] = utility\n",
    "                agent_vote_utilities[agent_num] = v\n",
    "                \n",
    "                \n",
    "                #record for the whole thing final a ratio type\n",
    "                individual = final_results[agent_num]\n",
    "\n",
    "                #takes in utility of this last vote, the history of which option numbers won and the preference profile over numbers (util of vote) to determine agent behaviour\n",
    "                if A_type == \"reward\":\n",
    "                    thiswin = entire_winner_history[eps_number]\n",
    "                    A = util_of_vote[thiswin]\n",
    "                else:\n",
    "                    A = determine_A_type_from_utility_of_vote_assigned(utility,utility_last,winner_history, util_of_vote,A_type,poll_cutoff)\n",
    "\n",
    "                individual[eps_number] += A    \n",
    "    final_results = [a/len(history) for a in final_results]\n",
    "    return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "plt.figure(figsize=(15,15))\n",
    "opp = analyse_action_type(sample,'opportunity')\n",
    "stay = analyse_action_type(sample,'stay')\n",
    "comp = analyse_action_type(sample,'compromise')\n",
    "\n",
    "def ratio_for_agent(opp,stay,comp,number,start,end):\n",
    "    opp,stay,comp = opp[number],stay[number],comp[number]\n",
    "    start = int(start*len(opp))\n",
    "    end = int(end*len(opp)) - 1\n",
    "    opp = round(np.mean(opp[start:end]),4)\n",
    "    stay = round(np.mean(stay[start:end]),4)\n",
    "    comp = round(np.mean(comp[start:end]),4)\n",
    "\n",
    "    print(\"{} - Opp: {}, stay: {}, comp: {}...  O/C: {}\".format(number,opp,stay,comp,round(opp/comp,2)))\n",
    "\n",
    "ags = len(opp)\n",
    "\n",
    "s = 0.9\n",
    "e = 1.0\n",
    "print(s,e)\n",
    "\n",
    "for agent in range(ags):\n",
    "    ratio_for_agent(opp=opp,stay=stay,comp=comp,number=agent,start=s,end=e)\n",
    "\n",
    "s = 0.0\n",
    "e = 0.1\n",
    "\n",
    "print(s,e)\n",
    "\n",
    "for agent in range(ags):\n",
    "    ratio_for_agent(opp=opp,stay=stay,comp=comp,number=agent,start=s,end=e)\n",
    "\n",
    "#ARE THESE RATES JUST DETERMINED BY EPS??\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_starts = {}\n",
    "A_ends = {}\n",
    "testfrac = 0.1\n",
    "A_list = ['reward','TRT','LB','DLB','CMP','CLB','DOM']\n",
    "POLC=20\n",
    "\n",
    "for A in A_list:\n",
    "    A_result = analyse_action_type(sample,A,poll_cutoff=POLC)\n",
    "    interval = int(len(A_result[0])*testfrac) + POLC\n",
    "    minterval = int(-1*len(A_result[0])*testfrac)\n",
    "    agent_a_start = [np.mean(agent_a[POLC:interval]) for agent_a in A_result]\n",
    "    agent_a_end = [np.mean(agent_a[minterval:]) for agent_a in A_result]\n",
    "    A_starts[A] = tuple(agent_a_start)\n",
    "    A_ends[A] = tuple(agent_a_end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE = []\n",
    "for k in A_list:\n",
    "    AE.append(np.mean(A_ends[k]))\n",
    "\n",
    "AS = []\n",
    "for k in A_list:\n",
    "    AS.append(np.mean(A_starts[k]))\n",
    "\n",
    "x = np.arange(len(AS))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.bar(x,AS,tick_label=A_list,width=0.2,label='RL/simultaneous, {}-{}'.format(POLC,POLC+interval))\n",
    "plt.bar(x+0.2,AE,tick_label=A_list,width=0.2,label='RL/simultaneous, {}-{}'.format(minterval+epslen,epslen))\n",
    "paper_vals = [0,0.7,0.48,0.12,0.48,0.72,0.02]\n",
    "plt.bar(x+0.4,paper_vals,width=0.2,label='Humans/sequential, 0-5/10')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"RL Agents\")\n",
    "print(\"Modelling current seq as poll with cutoff = {}\".format(POLC))\n",
    "print(\"candidates: {}\\nvoters: {}\\nnumber of iterations: {}\\nnumber of preference profiles: {}\\nnumber of repeats per profile: {}\\ntotal number of runs: {}\\ntotal rounds: {}\\ntotal data points (runs * voters * eps length): {}\".format(\\\n",
    "    CV[0],CV[1],interval,N_pref,nt,int(N_pref*nt),int(N_pref*nt*interval),int(N_pref*nt*V*interval)))\n",
    "\n",
    "print(\"\\n\\nHumans - Meir 2020\")\n",
    "print(\"candidates: {}\\nvoters: {}\\nnumber of iterations: {}\\nnumber of preference profiles: {}\\nnumber of repeats per profile: {}\".format(\\\n",
    "    3,7,\"5-10\",6,\"?\"))\n",
    "\n",
    "#began with agents voting truthfully\n",
    "#voting was sequential - 'Actions_now', not 'poll' estimate\n",
    "#no tiebreak\n",
    "#linear rewards\n",
    "#terminated on convergence\n",
    "\n",
    "# Subjects could play up to 6 games in a sequence, each time with a different preference profile\n",
    "#2 with no cw, 2 with cw=pw, 2 with cw not plurality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(POLC,interval)\n",
    "A_starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store vote_histories"
   ]
  }
 ]
}